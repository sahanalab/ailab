{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px3LgWa3DDH_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the gridworld environment\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid = np.array([\n",
        "            [0, 0, 0, 1],  # Goal at (0, 3)\n",
        "            [0, -1, 0, 0],  # Wall with reward -1\n",
        "            [0, 0, 0, 0],\n",
        "            [0, 0, 0, 0]  # Start at (3, 0)\n",
        "        ])\n",
        "        self.start_state = (3, 0)\n",
        "        self.state = self.start_state\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start_state\n",
        "        return self.state\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        return self.grid[state] == 1 or self.grid[state] == -1\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        next_state = list(state)\n",
        "        if action == 0:  # Move up\n",
        "            next_state[0] = max(0, state[0] - 1)\n",
        "        elif action == 1:  # Move right\n",
        "            next_state[1] = min(3, state[1] + 1)\n",
        "        elif action == 2:  # Move down\n",
        "            next_state[0] = min(3, state[0] + 1)\n",
        "        elif action == 3:  # Move left\n",
        "            next_state[1] = max(0, state[1] - 1)\n",
        "        return tuple(next_state)\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.get_next_state(self.state, action)\n",
        "        reward = self.grid[next_state]\n",
        "        self.state = next_state\n",
        "        done = self.is_terminal(next_state)\n",
        "        return next_state, reward, done"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
        "        self.q_table = np.zeros((4, 4, 4))  # Q-values for each state-action pair\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            return random.randint(0, 3)  # Explore\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])  # Exploit\n",
        "\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        max_future_q = np.max(self.q_table[next_state])  # Best Q-value for next state\n",
        "        current_q = self.q_table[state][action]\n",
        "        # Q-learning formula\n",
        "        self.q_table[state][action] = current_q + self.learning_rate * (\n",
        "            reward + self.discount_factor * max_future_q - current_q\n",
        "        )"
      ],
      "metadata": {
        "id": "hlsEZnAh_o-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorld()\n",
        "agent = QLearningAgent()\n",
        "\n",
        "episodes = 1000  # Number of training episodes\n",
        "\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()  # Reset the environment at the start of each episode\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)  # Choose an action\n",
        "        next_state, reward, done = env.step(action)  # Take the action and observe next state, reward\n",
        "        agent.update_q_value(state, action, reward, next_state)  # Update Q-values\n",
        "        state = next_state  # Move to the next state"
      ],
      "metadata": {
        "id": "eXDFRGd7_wlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "done = False\n",
        "print(\"\\n Testing agent after trainig ... \\n\")\n",
        "\n",
        "while not done:\n",
        "    action = np.argmax (agent.q_table[state])\n",
        "    next_state, reward, done = env.step(action)\n",
        "    print(f\"State: {state}, Action: {action}, Reward: {reward}\")\n",
        "    state = next_state\n",
        "print(\"\\n Agent reached terminal State.\")"
      ],
      "metadata": {
        "id": "UDfW5l4e_2qj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "541dc4c1-c03c-41fa-8b25-0ae6e91a543e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Testing agent after trainig ... \n",
            "\n",
            "State: (3, 0), Action: 0, Reward: 0\n",
            "State: (2, 0), Action: 0, Reward: 0\n",
            "State: (1, 0), Action: 0, Reward: 0\n",
            "State: (0, 0), Action: 1, Reward: 0\n",
            "State: (0, 1), Action: 1, Reward: 0\n",
            "State: (0, 2), Action: 1, Reward: 1\n",
            "\n",
            " Agent reached terminal State.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming you have defined the number of states and actions\n",
        "\n",
        "num_states = 10  # Example number of states\n",
        "\n",
        "num_actions = 5  # Example number of actions\n",
        "\n",
        "# Initialize Q-table with zeros\n",
        "\n",
        "Q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Print initial Q-table\n",
        "\n",
        "print(\"Initial Q-Table:\")\n",
        "\n",
        "print(Q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSBv3ZXaAKBT",
        "outputId": "4abf8c31-5558-4239-ac26-80a37280d3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Q-Table:\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "# Initialize the Q-table\n",
        "\n",
        "num_states = env.observation_space.n\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "Q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Parameters\n",
        "\n",
        "total_episodes = 1000\n",
        "\n",
        "learning_rate = 0.8\n",
        "\n",
        "max_steps = 99\n",
        "\n",
        "gamma = 0.95\n",
        "\n",
        "epsilon = 1.0\n",
        "\n",
        "max_epsilon = 1.0\n",
        "\n",
        "min_epsilon = 0.01\n",
        "\n",
        "decay_rate = 0.01\n",
        "\n",
        "# The Q-learning algorithm\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "  state = env.reset()\n",
        "\n",
        "  step = 0\n",
        "\n",
        "done = False\n",
        "\n",
        "for step in range(max_steps):\n",
        "\n",
        "# Choose an action in the current world state (s)\n",
        "\n",
        "# First we randomize a number\n",
        "\n",
        "  exp_exp_tradeoff = random.uniform(0, 1)\n",
        "\n",
        "# If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "\n",
        "if exp_exp_tradeoff > epsilon:\n",
        "\n",
        "  action = np.argmax(Q_table[state,:])\n",
        "\n",
        "# Else doing a random choice --> exploration\n",
        "\n",
        "else:\n",
        "\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "# Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "\n",
        "new_state, reward, done, info = env.step(action)\n",
        "\n",
        "# Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "\n",
        "Q_table[state, action] = Q_table[state, action] + learning_rate * (reward + gamma * np.max(Q_table[new_state, :]) - Q_table[state, action])\n",
        "\n",
        "# Our new state is state\n",
        "\n",
        "state = new_state\n",
        "\n",
        "import gym\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
        "\n",
        "# Initialize the Q-table\n",
        "\n",
        "num_states = env.observation_space.n\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "Q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Parameters\n",
        "\n",
        "total_episodes = 1000\n",
        "\n",
        "learning_rate = 0.8\n",
        "\n",
        "max_steps = 99\n",
        "\n",
        "gamma = 0.95\n",
        "\n",
        "epsilon = 1.0\n",
        "\n",
        "max_epsilon = 1.0\n",
        "\n",
        "min_epsilon = 0.01\n",
        "\n",
        "decay_rate = 0.01\n",
        "\n",
        "# The Q-learning algorithm\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "  state = env.reset()\n",
        "\n",
        "  step = 0\n",
        "\n",
        "  done = False\n",
        "\n",
        "  for step in range(max_steps):\n",
        "\n",
        "    # Choose an action in the current world state (s)\n",
        "    # First we randomize a number\n",
        "    exp_exp_tradeoff = random.uniform(0, 1)\n",
        "\n",
        "    # If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "    if exp_exp_tradeoff > epsilon:\n",
        "      action = np.argmax(Q_table[state,:])\n",
        "    # Else doing a random choice --> exploration\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "\n",
        "    # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "    new_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "    Q_table[state, action] = Q_table[state, action] + learning_rate * (reward + gamma * np.max(Q_table[new_state, :]) - Q_table[state, action])\n",
        "\n",
        "    # Our new state is state\n",
        "    state = new_state\n",
        "\n",
        "    # If done : finish episode\n",
        "    if done == True:\n",
        "      break;\n",
        "\n",
        "  # Reduce epsilon (because we need less and less exploration)\n",
        "  epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
        "\n",
        "# Print the Q-table\n",
        "print(\"Q-table:\")\n",
        "print(Q_table)\n",
        "# Reduce epsilon (because we need less and less exploration)\n",
        "\n",
        "epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
        "\n",
        "# Print the Q-table\n",
        "\n",
        "print(\"Q-table:\")\n",
        "\n",
        "print(Q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8cbEMfwAM_z",
        "outputId": "8fdf3138-7751-4d6c-ee04-a798059b3e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "Q-table:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}